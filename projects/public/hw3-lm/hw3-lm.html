<!DOCTYPE html>
<meta charset="utf-8"/>
<html>

<head>
    <title>CS147 - Deep Learning | Brown University</title>

    <!-- NOTE: Template needs absolute paths since actual created files may be in different subdirectories  -->
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,700,700i&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="http://cs.brown.edu/courses/cs1470/style.css">

    <!-- for syntax highlighting of code blocks -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/languages/go.min.js"></script>
    <script>
        hljs.initHighlightingOnLoad();
    </script>

    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <!-- NOTE: Script closing tags need to be on separate line for markdown-to-html script to process them properly :-(  -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js">
    </script>
    <script type="text/javascript" src="http://cs.brown.edu/courses/cs1470/random-bowties.js">
    </script>
    <script type="text/javascript" src="http://cs.brown.edu/courses/cs1470/create-sidebar.js">
    </script>
     <script type="text/javascript" src="http://cs.brown.edu/courses/cs1470/common.js">
     </script>

</head>

<body>
    <header class="lemons-bg">
        <div id="navbar" class="fixed">
            <ul>
                <li><a href="http://cs.brown.edu/courses/cs1470/index.html">Home</a></li>
                <li><a href="http://cs.brown.edu/courses/cs1470/resources.html">Resources</a></li>
                <li><a href="http://cs.brown.edu/courses/cs1470/lectures.html">Lectures</a></li>
                <li><a href="http://cs.brown.edu/courses/cs1470/assignments.html">Assignments</a></li>
                <li><a href="http://cs.brown.edu/courses/cs1470/labs.html">Labs</a></li>
                <li><a href="http://cs.brown.edu/courses/cs1470/calendar.html">Calendar &amp; Hours</a></li>
                <li><a href="http://cs.brown.edu/courses/cs1470/staff.html">Staff</a></li>
            </ul>
        </div>
    </header>
    <main>
        <section class="has-sidebar">
            <h1 id="hw3languagemodels">HW3: Language Models</h1>

            <p>Due <strong>Wednesday, 10/23/19 at 11:59pm</strong></p>

            <p>This sentence is words. These words have meaning. How can we make a computer make words with meaning?</p>

            <p>We will be building a model that can generate language just as coherent as this as coherent as this as coherent as this is!</p>

            <h2 id="gettingthestencil">Getting the stencil</h2>

            <p>You can find the files located <a href="http://cs.brown.edu/courses/cs1470/projects/public/hw3-lm/stencil_and_data.zip" download>here</a> or on the "Files" column under the Assignments page. The files are compressed into a ZIP file, and to unzip the ZIP file, you can just double click on the ZIP file. There should be the files: rnn.py, trigram.py, preprocess.py, README, and data folder.
            You can find the conceptual questions located <a href="http://cs.brown.edu/courses/cs1470/projects/public/hw3-lm/hw3-conceptual-q.pdf">here</a> or the "Conceptual Questions" column in the Assignments page.</p>

            <h2 id="logistics">Logistics</h2>

            <p>Work on this assignment off of the stencil code provided, but <strong>do not change the stencil except where specified.</strong> Changing the stencil will result in incompatiblity with the autograder and result in a low grade. You shouldn't change any method signatures.</p>

            <p>This assignment has <strong>2 main parts.</strong> You will implement two different types of language model and train them on the modified Wall Street Journal corpus we have provided. They will share a preprocessing file. You must also answer language model conceptual questions.</p>

            <h2 id="thecorpus">The Corpus</h2>

            <p>When it comes to language modeling, there is a LOT of data out there! In language modeling, we take some set of words, and train our model to predict the next, so almost any large text corpus will do. </p>

            <p>Our data comes from the WSJ corpus, and comprises sentences taken from the newspaper--in fact, if you look at the text, you will see lots of references to finance, stocks, etc. Another thing you will see in the corpus is things that look like this: <em>UNK-</em>.</p>

            <p>Uncommon words have been <em>UNKed.</em> This is a common preprocessing technique to remove rare words, and help the model focus more on learning patterns in general, common language. Because of this 'UNKing' there are no words in the testing corpus that are not in the training corpus!</p>

            <p>Lastly, you will notice that each text file has a sentence per line, followed by a 'STOP'. For this assignment, we don't need to worry about where some sentences end and others begin. Thus, in preprocessing, you should concatenate all the words in the sentences together. (Doing this will mean that you can avoid padding and masking, but have no fear--these will figure into our next assignment!).</p>

            <h1 id="part0preprocessing">Part 0: Preprocessing</h1>

            <p>Your preprocessing file should contain a <code>get_data</code> function. This function takes a training file and testing file. </p>

            <p>In this function you should:</p>

            <p>Step 1. Load the train words and split the words on whitespace.</p>

            <p>Step 2. Load the test words and split the words on whitespace.</p>

            <p>Step 3. Create a vocab dict that maps a unique index (id) to each word in the corpus</p>

            <p>Step 4. Convert the list of training and test words to their indeces, making a 1-d list/array for each</p>

            <p>Step 5. Return an iterable of training ids, an iterable of testing ids, and the vocab dict</p>

            <h1 id="part1trigramlanguagemodel">Part 1: Trigram Language Model</h1>

            <h2 id="roadmap">Roadmap</h2>

            <p>In the Trigam Language Model Part of the assignment, you will build a neural network that takes two words and predicts the third. It should do this by looking up the input words in the embedding matrix, and feeding the result into a set of feed-forward layers.</p>

            <p>Step 1. Create your model</p>

            <ul>
            <li>Fill out the init function, and define your trainable variables. </li>

            <li>Fill out the call function using the trainable variables you've created.</li>

            <li>Calculate the softmax cross-entropy loss on the probabilities compared to the labels (These should NOT be one hot vectors). We recommend using <code>tf.keras.losses.sparse_categorical_crossentropy</code>.</li>
            </ul>

            <p>Step 2. Train and test</p>

            <ul>
            <li>In the main function, you will want to get your train and test data, initialize your model, and train it for <strong>1 epoch</strong>. We have provided you with a train and test method to fill out. The train method will take in the model and do the forward and backward pass.</li>
            </ul>

            <h2 id="mandatoryhyperparameters">Mandatory Hyperparameters</h2>

            <p>You must use a single embedding matrix, which you query using <code>tf.nn.embedding_lookup</code>. You should look up the embeddings for your two words, concatenate them, and then feed the result to a few linear layers. Also, remember that there should be a nonlinear function (in our case RelU), applied between the feed-forward layers.</p>

            <p>Remember that your embedding matrix should store a unique vector for every word in the corpus.
            In addition, your call function should return a probability for every word in your vocab.</p>

            <p><strong>While the specifications for your architecture are flexible, your trigram model must train in under 10 minutes on a department machine!</strong></p>

            <h2 id="seeingresults">Seeing Results</h2>

            <ul>
            <li>We've provided a function for you to see text generated from your trained model.</li>
            </ul>

            <h1 id="part2rnnlanguagemodelwithkeraslayers">Part 2: RNN Language Model...with Keras Layers!</h1>

            <p>Now that we are working with more complex architectures, like LSTMs and GRUs, it's time to add in a little more abstraction. Going forward, unless otherwise specified, you can use <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">keras layers</a> in your model. For these assignment, the relevant layers are: </p>

            <ul>
            <li><code><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU">tf.keras.layers.GRU</a></code> OR <code><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM">tf.keras.layers.LSTM</a></code> for your RNN</li>

            <li><code><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">tf.keras.layers.Dense</a></code> for feedforward layers.</li>
            </ul>

            <p>You can define this layers in your init, then call them on your inputs in your model.call function.</p>

            <ul>
            <li>Note: Do not use the sequential API.</li>
            </ul>

            <h2 id="roadmap-1">Roadmap</h2>

            <p>Step 1. Create your model</p>

            <ul>
            <li>Fill out the init function, and define your trainable variables. As for trigrams, this will include an embedding matrix. However, for this part assignment, you will now want to define an rnn keras layer, along with dense layers.</li>
            </ul>

            <p><strong>When you define your keras layer, make sure that initialize it with <code>return_sequences=True</code>, <code>return_state=True</code></strong></p>

            <ul>
            <li>Fill out the call function using the trainable variables you've created.</li>
            </ul>

            <p><strong>Note that the final state from an LSTM has two components, the last output and cell state. These are the second and third outputs of calling the RNN. If you use a GRU, the final state is the second output returned by the RNN layer.</strong></p>

            <p>Additionally, you may want to include logic for handling when the initial state is None.</p>

            <ul>
            <li>Calculate the average softmax cross-entropy loss on the probabilities compared to the labels. Again, we suggest using <code>tf.keras.losses.sparse_categorical_crossentropy</code>.</li>
            </ul>

            <p>Step 2. Train and test</p>

            <ul>
            <li><p>In the main function, you will want to get your train and test data, initialize your model, and train it for <strong>1 epoch</strong>. We have provided for you a train and test method to fill out. The train method will take in the model and do the forward and backward pass.</p></li>

            <li><p><strong>You do not need to worry about passing your hidden state between batches!</strong></p></li>
            </ul>

            <h2 id="mandatoryhyperparameters-1">Mandatory Hyperparameters</h2>

            <p>While you can train with any batch size, you must use a window size of 20, and can train for at most 1 epoch.</p>

            <p>Again, the specifications for this assignment are weak. Your must have an embedding layer, followed by an RNN.</p>

            <p><strong>However, your models must train in under 10 minutes on a department machine!</strong></p>

            <h2 id="seeingresults-1">Seeing Results</h2>

            <ul>
            <li>We've provided a generation function for this part of the assignment. You can pass your model to this function to see sample generations. If you see lot's of <em>UNKs</em>, have no fear! These are common symbols in the corpus.</li>
            </ul>

            <h1 id="part3conceptualquestions">Part 3: Conceptual Questions</h1>

            <p><strong>Fill out conceptual questions and submit in PDF format.</strong> Submitting a scan of written work is also fine as long as it is readable. We had many issues opening up non PDFs while grading and will not allow for fixes after the fact. Please copy over the questions and write well thought out answers to the questions.</p>

            <h1 id="plspdf">Please submit your conceptual questions as a PDF.</h1>

            <h2 id="cs2470students">CS2470 Students</h2>

            <p>Please complete the CS2470-only conceptual questions <strong>in addition</strong> to the coding assignment and the CS1470 conceptual questions.
            <strong>Note: Questions about 2470 will only be answered on Piazza, or by TAs marked with an asterisk (*) on the calendar.</strong></p>

            <h1 id="grading">Grading</h1>

            <p><strong>Code:</strong> You will be primarily graded on functionality. <strong>Your Trigram model should have a test perplexity &lt; 280, and your RNN model should have a perplexity &lt; 150.</strong></p>

            <p><strong>Conceptual:</strong> You will be primarily graded on correctness (when applicable), thoughtfulness, and clarity. </p>

            <h2 id="autograder">Autograder</h2>

            <p>Your model must complete training within 10 minutes for your Trigram Model, and 10 minutes for the RNN.</p>

            <p>Our autograder will import your model and your preprocessing functions. We will feed the result of your <code>get_data</code> function called on a path to our data and pass the result to your train method in order to return a fully trained model. We will test using our testing function.

            <h1 id="handingin">Handing In</h1>

            <p>You should submit the assignment using <a href="https://docs.google.com/forms/d/e/1FAIpQLSe8oRO1a1g6iEW3ixpjmtzL9-da4No-ZrOmduZyv7P904LvUw/viewform">this Google Form</a>. You must be logged in with your Brown account. Your assignment.py, preprocess.py, trigam.py, and rnn.py files should be Python files, while the written up conceptual questions should be either of PDF or txt format. The README can be any format.</p>
        </section>
        <aside style="top:50px">
        </aside>
    </main>

    <footer class="dark-footer">
        <img id="footer-earmuffs" class="random-earmuffs" src="http://cs.brown.edu/courses/cs1470/img/earmuffs_1.png">
        <ul class="menu">
            <li>&copy; 2019 CS1470/2470 TA Staff | Computer Science Department | Brown University</li>
        </ul>
        <br>
    </footer>

</body>

</html>
